# è¯·å…ˆå®‰è£…ä¾èµ–ï¼špip install streamlit openai langchain-community faiss-cpu pypdf python-docx pdfminer.six docx2txt tiktoken

import streamlit as st
from openai import OpenAI
import os
import tempfile
import time
from tenacity import retry, stop_after_attempt, wait_fixed
from typing import List, Union, Optional
from langchain.embeddings.base import Embeddings
from langchain.document_loaders import UnstructuredFileLoader

# LangChain ç»„ä»¶
try:
    from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
    from langchain_community.vectorstores import FAISS
except ImportError:
    from langchain.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
    from langchain.vectorstores import FAISS

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document

# åˆå§‹åŒ–é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½çŸ¥è¯†åº“åŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="centered"
)


# è‡ªå®šä¹‰åµŒå…¥æ¨¡å‹ï¼ˆå·²ä¿®å¤æ‰¹é‡é™åˆ¶é—®é¢˜ï¼‰
class QwenEmbeddings(Embeddings):
    """é€šä¹‰åƒé—®ä¸“ç”¨åµŒå…¥æ¨¡å‹ï¼ˆæ”¯æŒåˆ†æ‰¹å¤„ç†ï¼‰"""

    def __init__(self, api_key: str, model: str = "text-embedding-v3", batch_size: int = 10):
        if not api_key:
            raise ValueError("API key is required for QwenEmbeddings")
        self.client = OpenAI(
            api_key=api_key,
            base_url=st.secrets["QWEN_BASE_URL"]
        )
        self.model = model
        self.batch_size = min(batch_size, 10)  # å¼ºåˆ¶éµå®ˆAPIé™åˆ¶

    def embed_documents(self, texts: List[str], dimensions: int = 1024) -> List[List[float]]:
        """åˆ†æ‰¹å¤„ç†æ–‡æ¡£åµŒå…¥"""
        embeddings = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            response = self.client.embeddings.create(
                model=self.model,
                input=batch,
                dimensions=dimensions,
                encoding_format="float"
            )
            embeddings.extend([data.embedding for data in response.data])
        return embeddings

    def embed_query(self, text: str, dimensions: int = 1024) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
        response = self.client.embeddings.create(
            model=self.model,
            input=[text],
            dimensions=dimensions,
            encoding_format="float"
        )
        return response.data[0].embedding


# åˆå§‹åŒ–å®¢æˆ·ç«¯
@st.cache_resource
def get_openai_client():
    return OpenAI(
        api_key=st.secrets["DEEPSEEK_API_KEY"],
        base_url=st.secrets.get("DEEPSEEK_BASE_URL", "https://api.deepseek.com/beta")
    )


client = get_openai_client()


# ==================== çŠ¶æ€ç®¡ç† ====================
def init_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    defaults = {
        "messages": [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"}],
        "vector_store": None,
        "kb_ready": False,
        "kb_version": 0,
        "last_search_time": None
    }
    for key, val in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = val


init_session_state()


# ==================== æ–‡ä»¶å¤„ç† ====================
@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
def load_document(file_path: str, file_type: str) -> List[Document]:
    """åŠ è½½å•ä¸ªæ–‡æ¡£"""
    loader_map = {
        "application/pdf": PyPDFLoader,
        "text/plain": TextLoader,
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document": Docx2txtLoader
    }
    loader_class = loader_map.get(file_type)
    if not loader_class:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
    return loader_class(file_path).load()


def process_files(uploaded_files):
    """å¤„ç†ä¸Šä¼ æ–‡ä»¶ï¼ˆå·²ä¿®å¤å…ƒæ•°æ®å†²çªï¼‰"""
    try:
        # é¢„æ£€æŸ¥NLTKèµ„æº
        try:
            from nltk import data
            data.find('tokenizers/punkt')
        except LookupError:
            import nltk
            nltk.download('punkt')

        documents = []
        for file in uploaded_files:
            with tempfile.NamedTemporaryFile(delete=False, suffix=file.name) as tmp:
                tmp.write(file.getvalue())
                tmp_path = tmp.name

            try:
                file_type = file.type
                if file_type == "application/pdf":
                    loader = PyPDFLoader(tmp_path)
                elif file_type == "text/plain":
                    loader = TextLoader(tmp_path)
                elif file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    loader = UnstructuredFileLoader(tmp_path, mode="elements")
                else:
                    st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
                    continue

                loaded_docs = loader.load()
                for doc in loaded_docs:
                    doc.metadata.update({
                        "source": file.name,
                        "file_type": file_type,
                        "upload_time": time.strftime("%Y-%m-%d %H:%M:%S")
                    })
                documents.extend(loaded_docs)
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶ {file.name} å¤±è´¥: {str(e)}")
                if "punkt" in str(e):
                    st.error("è¯·è¿è¡Œ: python -m nltk.downloader punkt")
            finally:
                os.unlink(tmp_path)

        if not documents:
            st.error("æ²¡æœ‰å¯å¤„ç†çš„æ–‡æ¡£å†…å®¹")
            return

        # æ–‡æœ¬åˆ†å—å¤„ç†
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            add_start_index=True
        )
        chunks = text_splitter.split_documents(documents)

        if not chunks:
            st.error("æ–‡æ¡£åˆ†å‰²åæ— æœ‰æ•ˆå†…å®¹")
            return

        # ç”Ÿæˆå‘é‡å­˜å‚¨ï¼ˆå·²ä¿®å¤æ‰¹é‡é™åˆ¶ï¼‰
        with st.spinner("æ­£åœ¨ç”ŸæˆçŸ¥è¯†åº“..."):
            api_key = st.secrets["QWEN_API_KEY"]
            embeddings = QwenEmbeddings(
                api_key=api_key,
                batch_size=10  # æ˜¾å¼è®¾ç½®æ‰¹å¤„ç†å¤§å°
            )

            st.session_state.vector_store = FAISS.from_documents(
                chunks,
                embeddings
            )

            st.session_state.kb_ready = True
            st.session_state.kb_version = hash(tuple(f.name for f in uploaded_files))
            st.success("çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸï¼")

    except Exception as e:
        st.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")
        st.session_state.vector_store = None
        st.session_state.kb_ready = False


# ==================== æ£€ç´¢åŠŸèƒ½ ====================
def is_kb_valid() -> bool:
    """éªŒè¯çŸ¥è¯†åº“æ˜¯å¦æœ‰æ•ˆ"""
    return bool(
        st.session_state.kb_ready and
        st.session_state.vector_store and
        hasattr(st.session_state.vector_store, "similarity_search")
    )


@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
def safe_retrieval(query: str, k: int = 3) -> List[Document]:
    """å¸¦é”™è¯¯å¤„ç†çš„æ£€ç´¢æ–¹æ³•"""
    if not is_kb_valid():
        raise ValueError("çŸ¥è¯†åº“æœªå°±ç»ª")

    start_time = time.time()
    docs = st.session_state.vector_store.similarity_search(query, k=k)
    st.session_state.last_search_time = time.time() - start_time
    return docs


# ==================== ç•Œé¢ç»„ä»¶ ====================
def show_sidebar():
    """ä¾§è¾¹æ ç»„ä»¶"""
    with st.sidebar:
        st.header("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")

        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ æ–‡æ¡£ï¼ˆPDF/TXT/DOCXï¼‰",
            type=["pdf", "txt", "docx"],
            accept_multiple_files=True
        )

        if uploaded_files:
            process_files(uploaded_files)

        # çŸ¥è¯†åº“çŠ¶æ€
        if st.session_state.kb_ready:
            st.success("âœ… çŸ¥è¯†åº“å·²å°±ç»ª")
            try:
                st.caption(f"çŸ¥è¯†ç‰‡æ®µæ•°é‡: {st.session_state.vector_store.index.ntotal}")
            except:
                pass

            if st.button("ğŸ”„ æ¸…é™¤çŸ¥è¯†åº“"):
                st.session_state.vector_store = None
                st.session_state.kb_ready = False
                st.session_state.kb_version = 0
                st.rerun()

        # æ€§èƒ½ç›‘æ§
        if st.session_state.last_search_time:
            st.divider()
            st.metric("ä¸Šæ¬¡æ£€ç´¢è€—æ—¶", f"{st.session_state.last_search_time:.2f}s")


def chat_interface():
    """ä¸»èŠå¤©ç•Œé¢"""
    st.title("ğŸ’¬ æ™ºèƒ½çŸ¥è¯†åº“åŠ©æ‰‹")

    # æ¨¡å¼åˆ‡æ¢
    mode = st.radio(
        "æ¨¡å¼é€‰æ‹©",
        ["ğŸ’¬ æ™®é€šèŠå¤©", "ğŸ“š çŸ¥è¯†åº“é—®ç­”"],
        horizontal=True,
        index=1 if st.session_state.kb_ready else 0
    )

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages[1:]:  # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ç”¨æˆ·è¾“å…¥å¤„ç†
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š"):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        # å‡†å¤‡å“åº”
        with st.chat_message("assistant"):
            try:
                # çŸ¥è¯†åº“æ¨¡å¼å¤„ç†
                if mode == "ğŸ“š çŸ¥è¯†åº“é—®ç­”":
                    if not is_kb_valid():
                        st.error("çŸ¥è¯†åº“æœªå°±ç»ªï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
                        st.stop()

                    # æ£€ç´¢ä¸Šä¸‹æ–‡
                    try:
                        docs = safe_retrieval(prompt, k=3)
                        context = "\n\n".join(
                            [f"**[æ¥æºï¼š{doc.metadata['source']}]**\n{doc.page_content}"
                             for doc in docs]
                        )
                        enhanced_prompt = f"è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜ï¼š{prompt}"
                    except Exception as e:
                        st.error(f"æ£€ç´¢å¤±è´¥: {str(e)}")
                        st.stop()
                else:
                    enhanced_prompt = prompt

                # æµå¼è¾“å‡º
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[{"role": m["role"], "content": m["content"]}
                              for m in st.session_state.messages[:-1]] +
                             [{"role": "user", "content": enhanced_prompt}],
                    stream=True
                )

                full_response = ""
                placeholder = st.empty()
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        placeholder.markdown(full_response + "â–Œ")
                placeholder.markdown(full_response)

                # æ›´æ–°æ¶ˆæ¯å†å²
                st.session_state.messages.append(
                    {"role": "assistant", "content": full_response}
                )

            except Exception as e:
                st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")


# ==================== ä¸»ç¨‹åº ====================
show_sidebar()
chat_interface()
